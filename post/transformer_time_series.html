<!DOCTYPE html>
<html>
<head>
	<title>An implementation of transformer-based time-series forecasting, inspired by TimesFM</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="favicon.ico?">
	<link rel="apple-touch-icon" href="apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

</head>
<body>

	<section class="blog centering">
		<h1>An implementation of transformer-based time-series forecasting, inspired by TimesFM</h1>

		<hr class="squiggly-line">
		<p>
			This post is meant to serve as an introduction to transformer-based time series prediction with PyTorch, strongly inspired by the recent Google model TimesFM. As a technical reading experience, it is meant to fall somewhere between reading the original paper <a target="_blank" href="https://arxiv.org/abs/2310.10688">"A decoder-only foundation model for time-series forecasting"</a> and trying to read the <a target="_blank" href="https://github.com/google-research/timesfm">source code</a>. Specifically, we will build out a simpler version of the TimesFM model step-by-step, starting from setting up dataloaders and ending with plotting autoregressive forecasting. Note that this is not meant as a general introduction to either PyTorch nor time-series forecasting and what is included is mostly a reflection of where I myself struggled.
		</p>

		<p>
			As any in-vogue method, transformers for time-series are not without a counter-movement (rightly) questioning the effectiveness and strength of baselines used. Discussing this is decidedly a non-goal of this post and we try to stick to implementation details. Nonetheless, the context provided by an evaluation setup is valuable to properly define the problem we are trying to solve. For this reason, we begin by implementing a very simple model for time series forecasting based on an MLP block which will act as our baseline.
		</p>

		<hr class="squiggly-line">
		<h2>Dataset</h2>

		<p>
			We will use a standard weather dataset in CSV form available from <a target="_blank" href="https://www.kaggle.com/datasets/muthuj7/weather-dataset">Kaggle</a>, the first 5 rows of which can be seen below.
		</p>

		<table border="1" class="dataframe">
			<thead>
				<tr style="text-align: right;">
					<th>Formatted Date</th>
					<th>Summary</th>
					<th>Precip Type</th>
					<th>Temperature (C)</th>
					<th>Apparent Temperature (C)</th>
					<th>Humidity</th>
					<th>Wind Speed (km/h)</th>
					<th>Wind Bearing (degrees)</th>
					<th>Visibility (km)</th>
					<th>Loud Cover</th>
					<th>Pressure (millibars)</th>
					<th>Daily Summary</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>2006-04-01 00:00:00.000 +0200</td>
					<td>Partly Cloudy</td>
					<td>rain</td>
					<td>9.472222</td>
					<td>7.388889</td>
					<td>0.89</td>
					<td>14.1197</td>
					<td>251.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1015.13</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 01:00:00.000 +0200</td>
					<td>Partly Cloudy</td>
					<td>rain</td>
					<td>9.355556</td>
					<td>7.227778</td>
					<td>0.86</td>
					<td>14.2646</td>
					<td>259.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1015.63</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 02:00:00.000 +0200</td>
					<td>Mostly Cloudy</td>
					<td>rain</td>
					<td>9.377778</td>
					<td>9.377778</td>
					<td>0.89</td>
					<td>3.9284</td>
					<td>204.0</td>
					<td>14.9569</td>
					<td>0.0</td>
					<td>1015.94</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 03:00:00.000 +0200</td>
					<td>Partly Cloudy</td>
					<td>rain</td>
					<td>8.288889</td>
					<td>5.944444</td>
					<td>0.83</td>
					<td>14.1036</td>
					<td>269.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1016.41</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 04:00:00.000 +0200</td>
					<td>Mostly Cloudy</td>
					<td>rain</td>
					<td>8.755556</td>
					<td>6.977778</td>
					<td>0.83</td>
					<td>11.0446</td>
					<td>259.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1016.51</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
			</tbody>
		</table>

	<p>
		From it we see that we have readings for a few weather parameters with a one hour interval. In the interest of simplicity, we restrict ourselves to just the temperature for now which is in the column with index <code>3</code>. We can write a <code>PyTorch Dataset</code> for this as follows. 
	</p>

	<div class="code_block" id="code_weather_dataset"></div>

	<script type="module">
	    import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class WeatherDataset(Dataset):
    def __init__(self, ):
        self.frame = pd.read_csv("data/data.csv")

    def __len__(self):
        return len(self.frame)

    def __getitem__(self, idx):
        data = self.frame.iloc[idx, 3]
            
        return torch.tensor(data, dtype=torch.float32)
    `;

		const foo = document.getElementById('code_weather_dataset');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		With the weather dataset abstracted away, we can write another dataset for the actual prediction task. It should be able to do a few key things:
	</p>
		<ul>
			<li>Indicate <code>train/test</code> split, separated by an appropriate buffer zone.</li>
			<li>Get a <code>series</code> tensor of length <code>context_len</code> which should be the input to the model, and a <code>target</code> tensor of length <code>output_len</code> which is the desired output which we will use for the loss function.</li> 
		</ul>
		<p>
		We choose for the test set to start after 80% of the data points and a buffer zone of twice the <code>context_len</code>.
	</p>
	<div class="code_block" id="code_timeseries_dataset"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class TimeSeriesDataset(Dataset):
    def __init__(self, points_ds, context_len, output_len, split='train'):
        full_length = len(points_ds)
        test_start = floor(full_length * 0.8)
        train_stop = test_start - context_len * 2
        
        if split == 'train':
            train_len = train_stop
            self.points = [points_ds[i] for i in range(train_len)]
        else:
            self.points = [points_ds[i] for i in range(test_start, full_length)]
        
        self.points = torch.stack(self.points)
        
        self.context_len = context_len
        self.output_len = output_len
        
    def __len__(self):
        return len(self.points) - self.context_len - self.output_len
    
    def __getitem__(self, idx):
        series = self.points[idx : idx + self.context_len]
        target = self.points[idx + self.context_len :
                             idx + self.context_len + self.output_len]
                        
        return series, target
    `;

		const foo = document.getElementById('code_timeseries_dataset');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		Below is an example with <code>context_len = 2048</code> and <code>output_len = 128</code>.
	</p>

	<hr class="squiggly-line">
	<h2>MLP baseline and model generalities</h2>

	<p>
		Now we turn to developing the simple multilayer-perceptron model for the baseline. Our input data, the temperatures from the table, is not dimensionless and can significantly drift over time. For this reason it is beneficial to normalize each slice we let the model act on and then invert the normalization after the model has acted. To this end, we set up the following helper functions which takes an input <code>x</code>of shape <code>(batch_size, len)</code>:
	</p>

	<div class="code_block" id="code_normalization"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
def normalize(x, m = None, s = None):    
    if m is None or s is None:
        m = x.mean(dim=1)
        s = x.std(dim=1)
        
    return (x - m) / s, m, s

def un_normalize(x, m, s):        
    return (x * s) + m
    `;

		const foo = document.getElementById('code_normalization');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		The reason we allow the mean and standard deviations to be given as arguments is that we want future data to be normalized using the statistics from the context window. We use this in our loss function which - on account of this normalization - is scale invariant. As a basis we use the standard Mean Squared Error (MSE) \(\ell^2\)-loss function.
	</p>

	<div class="code_block" id="code_loss"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
def normalized_mse(series, pred, target):
    _, m, s = normalize(series)
        
    pred, _, _ = normalize(pred, m, s)
    target, _, _ = normalize(target, m, s)
        
    return nn.MSELoss()(pred, target)
    `;

		const foo = document.getElementById('code_loss');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		We can now set up the baseline model with a single hidden layer and a simple <code>ReLU</code> nonlinearity:
	</p>

	<div class="code_block" id="code_MLP"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class MLPForecast(nn.Module):
    def __init__(self, context_len, output_len):
        super().__init__()
                    
        hidden_dim = output_len * 4
        
        self.fc1 = nn.Linear(context_len, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_len)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x, m, s = normalize(x)
        
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
                
        x = un_normalize(x, m, s)
        
        return x
    `;

		const foo = document.getElementById('code_MLP');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		This is basically all that is needed for this simple model. Below is the training code which is standard with dynamic <code>device</code> selection and the <code>schedulefree.AdamWScheduleFree</code> optimizer from <a target="_blank" href="https://github.com/facebookresearch/schedule_free">Meta</a>.
	</p>

	<div class="code_block" id="code_MLP_train"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
if __name__ == '__main__':
    output_len = 128
    context_len = 1024
    
    learning_rate = 3e-4
    batch_size = 16
    max_epochs = 1

    device = 'cpu'

    if torch.cuda.is_available():
        device = 'cuda'
    elif torch.backends.mps.is_available():
        device = 'mps'
    
    weather_ds = WeatherDataset()

    ds_train = TimeSeriesDataset(weather_ds, context_len, output_len, split = 'train')
    ds_test = TimeSeriesDataset(weather_ds, context_len, output_len, split = 'test')

    dl_train = DataLoader(ds_train, batch_size = batch_size, shuffle = True)
    dl_test = DataLoader(ds_test, batch_size = batch_size, shuffle = True)
    
    model = MLPForecast(context_len, output_len)
    
    optimizer = schedulefree.AdamWScheduleFree(model.parameters(), lr=learning_rate)
    model = model.to(device)
    
    losses = []
    
    for epoch in range(max_epochs):
        print("--------Epoch {}--------".format(epoch + 1))
        train(model, device, optimizer, dl_train)
        test(model, device, optimizer, dl_test)

    print("Training completed")
    `;

		const foo = document.getElementById('code_MLP_train');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		The <code>train()</code> and <code>test()</code> functions with some <code>tqdm</code> and plotting niceties are defined as follows:
	</p>

	<div class="code_block" id="code_MLP_traintest"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
def train(model, device, optimizer, dataloader):
    model.train()
    optimizer.train()
    
    progress_bar = tqdm(dataloader, desc="Training", leave=True)
    cum_loss = 0

    for step, (series, target) in enumerate(progress_bar):
        optimizer.zero_grad()
        
        series = series.to(device)
        target = target.to(device)
                   
        pred = model(series).squeeze()
        
        loss = normalized_mse(series, pred, target)
        losses.append(loss.item())

        loss.backward()
        optimizer.step()
            
        cum_loss += loss.item()
        progress_bar.set_postfix(running_loss = cum_loss/(step + 1))
    
def test(model, device, optimizer, dataloader):
    model.eval()
    optimizer.eval()
    
    progress_bar = tqdm(dataloader, desc="Validating", leave=True)
    
    cum_loss = 0
    
    with torch.no_grad():
        for step, (series, target) in enumerate(progress_bar):           
            series = series.to(device)
            target = target.to(device)
            
            pred = model(series).squeeze()
            loss = normalized_mse(series, pred, target)
            
            cum_loss += loss.item()
            progress_bar.set_postfix(running_loss = cum_loss/(step + 1))
                    
    print("Validation MSE: {}".format(cum_loss/len(dataloader)))
    `;

		const foo = document.getElementById('code_MLP_traintest');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		Obviously
	</p>
	

	<hr class="squiggly-line">
	<h2>Transformers</h2>

	<p>
		While the transformer architecture was originally intended for NLP applications, it is nowadays seen more as a general compute engine to which we can attach our own encoders and decoders. There are endless variations on this "general compute engine" with different attention implementations and changes to the <code>Q, K, V</code> projections. We will treat the inner transformer block as a black box to some degree and focus on mapping to and from it.
	</p>

  	<p>
		Transformer blocks as we commonly know them are a self-map acting on a collection of <code>context_len</code> vectors of dimension <code>d_model</code>. As all neural networks, they work on batches so really it is tensors of shape <code>(batch_size, max_tokens, d_model)</code> which are mapped to new tensors of the same shape. Our goal will be to encode our times series of shape <code>(batch_size, context_len)</code> into a tensor of this form, allow a series of transformer blocks to act on it, and then decode the result into a tensor of shape <code>(batch_size, output_len)</code>.
	</p>

	<p>
		In NLP applications, words are split into tokens (<code>"ligma" -> ["lig", "ma"]</code>), each token is one-hot encoded and there is a learned embedding matrix which maps each token to a vector of dimension <code>d_model</code>. We could do the same for time series data by binning to map e.g. the interval \((0.1, 0.2)\) to a one-hot vector and then embedd that. Obviously, this approach ignores the inherent continuity of time series data, both in input and output. For image applications, the vision transformer (ViT) pioneered the idea of "patching" where the image is split into non-overlapping patches, each of which is mapped to a <code>d_model</code> vector through some <a target="_blank" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a>-like procedure. This is the approach we will use to encode our time series. Specifically, we will split our time series into <code>patches</code> patches, each of length <code>patch_len</code> so that <code>patches * patch_len = context_len.</code> In the TimesFM paper, this is done using a residual block.
	</p>
	<div class="code_block" id="code_res_block"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class ResidualBlock(nn.Module):
    def __init__(self,
                 input_dim,
                 output_dim,
                 hidden_dim,
                 dropout = 0.1,
                 apply_ln = True):
        super().__init__()

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.residual = nn.Linear(input_dim, output_dim)
        self.gelu = nn.GELU(approximate='tanh')
        self.dropout = nn.Dropout(dropout)
        
        self.apply_ln = apply_ln
        self.layer_norm = nn.LayerNorm(output_dim)
        
    def forward(self, x):
        residual = self.residual(x)
        
        x = self.fc1(x)
        x = self.gelu(x)
        x = self.fc2(x)
        x = self.dropout(x)
        
        x = x + residual
        if self.apply_ln:    
            x = self.layer_norm(x)
        
        return x
    `;

		const foo = document.getElementById('code_res_block');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		Now we can use the encoder to go from a <code>(batch_size, context_len)</code> to a <code>(batch_size, patches, patch_len)</code> tensor.
	</p>

	<div class="code_block" id="code_encoder"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
def encode_patches(self, x, patches, patch_len, encoder):
    x = x.view(x.shape[0], patches, patch_len)
    encoded_patches = []

    for i in range(patches):
        patch_data = x[:, i, :, :].flatten(start_dim=1)
        encoded_patch = encoder(patch_data)
        encoded_patches.append(encoded_patch)

    return encoded_patches
	`;

		const foo = document.getElementById('code_encoder');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>


	</section>




	<div class="centering">
		<hr class="squiggly-line">
	</div>
		

</body>
</html>