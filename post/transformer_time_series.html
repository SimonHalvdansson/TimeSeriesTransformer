<!DOCTYPE html>
<html>
<head>
	<title>An implementation of transformer-based time-series forecasting, inspired by TimesFM</title>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#FBEDEA" />

	<link rel="shortcut icon" type="image/x-icon"  href="favicon.ico?">
	<link rel="apple-touch-icon" href="apple-touch-icon.png">

	<link rel="stylesheet" href="style.css">

	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">

</head>
<body>

	<section class="blog centering">
		<h1>An implementation of transformer-based time-series forecasting, inspired by TimesFM</h1>

		<hr class="squiggly-line">
		<p>
			This post is meant to serve as an introduction to transformer-based time series prediction with PyTorch, strongly inspired by the recent Google model TimesFM. As a technical reading experience, it is meant to fall somewhere between reading the original paper "<a target="_blank" href="https://arxiv.org/abs/2310.10688">A decoder-only foundation model for time-series forecasting</a>" and trying to read the <a target="_blank" href="https://github.com/google-research/timesfm">source code</a>. Specifically, we will build out a simpler version of the TimesFM model step-by-step, starting from setting up dataloaders and ending with plotting autoregressive forecasting. Note that this is not meant as a general introduction to either PyTorch nor time-series forecasting and what is included is mostly a reflection of where I myself struggled.
		</p>

		<p>
			As any in-vogue method, transformers for time-series are not without a counter-movement (rightly) questioning the effectiveness and strength of baselines used. Discussing this is decidedly a non-goal of this post and we try to stick to implementation details. Nonetheless, the context provided by an evaluation setup is valuable to properly define the problem we are trying to solve. For this reason, we begin by implementing a very simple model for time series forecasting based on an MLP block which will act as our baseline.
		</p>

		<hr class="squiggly-line">
		<h2>Dataset</h2>

		<p>
			We will use a standard weather dataset in CSV form available from <a target="_blank" href="https://www.kaggle.com/datasets/muthuj7/weather-dataset">Kaggle</a>, the first 5 rows of which can be seen below.
		</p>

		<table border="1" class="dataframe">
			<thead>
				<tr style="text-align: right;">
					<th>Formatted Date</th>
					<th>Summary</th>
					<th>Precip Type</th>
					<th>Temperature (C)</th>
					<th>Apparent Temperature (C)</th>
					<th>Humidity</th>
					<th>Wind Speed (km/h)</th>
					<th>Wind Bearing (degrees)</th>
					<th>Visibility (km)</th>
					<th>Loud Cover</th>
					<th>Pressure (millibars)</th>
					<th>Daily Summary</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>2006-04-01 00:00:00.000 +0200</td>
					<td>Partly Cloudy</td>
					<td>rain</td>
					<td>9.472222</td>
					<td>7.388889</td>
					<td>0.89</td>
					<td>14.1197</td>
					<td>251.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1015.13</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 01:00:00.000 +0200</td>
					<td>Partly Cloudy</td>
					<td>rain</td>
					<td>9.355556</td>
					<td>7.227778</td>
					<td>0.86</td>
					<td>14.2646</td>
					<td>259.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1015.63</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 02:00:00.000 +0200</td>
					<td>Mostly Cloudy</td>
					<td>rain</td>
					<td>9.377778</td>
					<td>9.377778</td>
					<td>0.89</td>
					<td>3.9284</td>
					<td>204.0</td>
					<td>14.9569</td>
					<td>0.0</td>
					<td>1015.94</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 03:00:00.000 +0200</td>
					<td>Partly Cloudy</td>
					<td>rain</td>
					<td>8.288889</td>
					<td>5.944444</td>
					<td>0.83</td>
					<td>14.1036</td>
					<td>269.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1016.41</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
				<tr>
					<td>2006-04-01 04:00:00.000 +0200</td>
					<td>Mostly Cloudy</td>
					<td>rain</td>
					<td>8.755556</td>
					<td>6.977778</td>
					<td>0.83</td>
					<td>11.0446</td>
					<td>259.0</td>
					<td>15.8263</td>
					<td>0.0</td>
					<td>1016.51</td>
					<td>Partly cloudy throughout the day.</td>
				</tr>
			</tbody>
		</table>

	<p>
		From it we see that we have readings for a few weather parameters with a 1 hour interval. In the interest of simplicity, we restrict ourselves to just the temperature for now which is in the column with index <code>3</code>. We can write a <code>PyTorch Dataset</code> for this as follows. 
	</p>

	<div class="code_block" id="code_weather_dataset"></div>

	<script type="module">
	    import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class WeatherDataset(Dataset):
    def __init__(self, ):
        self.frame = pd.read_csv("data/data.csv")

    def __len__(self):
        return len(self.frame)

    def __getitem__(self, idx):
        data = self.frame.iloc[idx, 3]
            
        return torch.tensor(data, dtype=torch.float32)
    `;

		const foo = document.getElementById('code_weather_dataset');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		With the weather dataset abstracted away, we can write another dataset for the actual prediction task. It should be able to do a few key things:
	</p>
		<ul>
			<li>Indicate <code>train/test</code> split, separated by an appropriate buffer zone.</li>
			<li>Get a <code>series</code> tensor of length <code>context_len</code> which should be the input to the model, and a <code>target</code> tensor of length <code>output_len</code> which is the desired output which we will use for the loss function.</li> 
		</ul>
		<p>
		We choose for the test set to start after 80 % of the data points and a buffer zone of twice the <code>context_len</code>.
	</p>
	<div class="code_block" id="code_timeseries_dataset"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class TimeSeriesDataset(Dataset):
    def __init__(self, points_ds, context_len, output_len, split='train'):
        full_length = len(points_ds)
        test_start = floor(full_length * 0.8)
        train_stop = test_start - context_len * 2
        
        if split == 'train':
            train_len = train_stop
            self.points = [points_ds[i] for i in range(train_len)]
        else:
            self.points = [points_ds[i] for i in range(test_start, full_length)]
        
        self.points = torch.stack(self.points)
        
        self.context_len = context_len
        self.output_len = output_len
        
    def __len__(self):
        #remove context_len from __len__ so that we choose start points 
        #where target = idx + context_len + output_len is in self.points_ds
        return len(self.points) - self.context_len - self.output_len
    
    def __getitem__(self, idx):
        series = self.points[idx : idx + self.context_len]
        target = self.points[idx + self.context_len :
                             idx + self.context_len + self.output_len]
                        
        return series, target
    `;

		const foo = document.getElementById('code_timeseries_dataset');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<hr class="squiggly-line">
	<h2>MLP baseline and model generalities</h2>

	<p>
		Now we turn to developing the simple multilayer-perceptron model for the baseline. Our input data, the temperatures from the table, is not dimensionless and can significantly drift over time. For this reason it is beneficial to normalize each slice we let the model act on and then invert the normalization after the model has acted. To this end, we set up the following helper functions which takes inputs <code>x</code>of shape <code>(batch_size, len)</code>:
	</p>

	<div class="code_block" id="code_normalization"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
def normalize(x, m = None, s = None):    
    if m is None or s is None:
        m = x.mean(dim=1)
        s = x.std(dim=1)
        
    return (x - m) / s, m, s

def un_normalize(x, m, s):        
    return (x * s) + m
    `;

		const foo = document.getElementById('code_normalization');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		The reason we allow the mean and standard deviations to be given as arguments is that we want future data to be normalized using the statistics from the context window. We use this in our loss function which - on account of this normalization - is scale invariant. As a basis we use the standard Mean Squared Error (MSE) \(\ell^2\)-loss function.
	</p>

	<div class="code_block" id="code_loss"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
def normalized_mse(series, pred, target):
    _, m, s = normalize(series)
        
    pred, _, _ = normalize(pred, m, s)
    target, _, _ = normalize(target, m, s)
        
    return nn.MSELoss()(pred, target)
    `;

		const foo = document.getElementById('code_loss');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		We can now set up the baseline model with a single hidden layer and a simple <code>ReLU</code> nonlinearity:
	</p>

	<div class="code_block" id="code_MLP"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class MLPForecast(nn.Module):
    def __init__(self, context_len, output_len):
        super().__init__()
                    
        hidden_dim = output_len * 4
        
        self.fc1 = nn.Linear(context_len, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_len)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x, m, s = normalize(x)
        
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
                
        x = un_normalize(x, m, s)
        
        return x
    `;

		const foo = document.getElementById('code_MLP');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<p>
		This is basically all that is needed for this simple model. Below is the training code which is standard.
	</p>

		<div class="code_block" id="code_MLP_train"></div>

	<script type="module">
		import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class MLPForecast(nn.Module):
    def __init__(self, context_len, output_len):
        super().__init__()
                    
        hidden_dim = output_len * 4
        
        self.fc1 = nn.Linear(context_len, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_len)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x, m, s = normalize(x)
        
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
                
        x = un_normalize(x, m, s)
        
        return x
    `;

		const foo = document.getElementById('code_MLP_train');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>

	<hr class="squiggly-line">
	<h2>Transformers</h2>

  	<p>
		Transformer blocks as we commonly know them are a self-map acting on a collection of <code>context_len</code> vectors of dimension <code>d_model</code>. As all neural networks, they work on batches so really it is tensors of shape <code>(batch_size, max_tokens, d_model)</code> which are mapped to new tensors of the same shape. Our goal will be to encode our times series of shape <code>(batch_size, context_len)</code> into a tensor of this form, allow a series of transformer blocks to act on it, and then decode the result into a 
	</p>

	<p>
		In NLP applications, words are split into tokens (<code>"ligma" -> ["lig", "ma"]</code>), each token is one-hot encoded and there is a learned embedding matrix which maps each token to a vector of dimension <code>d_model</code>. We could do the same for time series data by binning to map e.g. the interval \((0.1, 0.2)\) to a one-hot vector and then embedd that. Obviously, this approach ignores the inherent continuity of time series data, both in input and output. For image applications, the vision transformer (ViT) pioneered the idea of "patching" where the image is split into non-overlapping patches, each of which is mapped to a <code>d_model</code> vector through some <a target="_blank" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a>-like procedure. This is the approach we will use to encode our time series.
	</p>

	</section>

		<script type="module">
	    import { codeToHtml } from 'https://esm.sh/shiki@1.0.0'
		const code = `
class WeatherDataset(Dataset):
    def __init__(self, ):
        self.frame = pd.read_csv("data/data.csv")

    def __len__(self):
        return len(self.frame)

    def __getitem__(self, idx):
        data = self.frame.iloc[idx, 3]
            
        return torch.tensor(data, dtype=torch.float32)
    `;

		const foo = document.getElementById('code_weather_dataset');
		foo.innerHTML = await codeToHtml(code, {
			lang: 'py',
			theme: 'rose-pine'
		});
	</script>


	<div class="centering">
		<hr class="squiggly-line">
	</div>
		
	<section class="papers centering">
		<h1>Papers</h1>

		<div class="masonry-grid">
			<div class="column">

				<div class="paper" id="accspec">
					<div class="formula">\(\boldsymbol{\rho_\Omega - \chi_\Omega}\)</div>
					<div class="title">On accumulated spectrograms for Gabor frames</div>
					<div class="subtitle">May 2024</div>
					<a href="https://arxiv.org/abs/2405.02059" target="_blank">ArXiV</a>
				</div>

				<div class="paper" id="measureoperatorconv">
					<div class="formula">\(\boldsymbol{\mu \star S}\)</div>
					<div class="title">Measure-operator convolutions and applications to mixed-state Gabor multipliers</div>
					<div class="subtitle">July 2024<br>with Hans Feichtinger and Franz Luef</div>
					<a href="https://link.springer.com/article/10.1007/s43670-024-00090-0" target="_blank">Sampling Theory, Signal Processing, and Data Analysis</a>
					<a href="https://arxiv.org/abs/2308.04985" target="_blank">ArXiV</a>
				</div>

				<div class="paper" id="qhalc">
					<div class="formula">\(\boldsymbol{f \star_G S}\)</div>
					<div class="title">Quantum harmonic analysis on locally compact groups</div>
					<div class="subtitle">October 2022</div>
					<a href="https://www.sciencedirect.com/science/article/pii/S0022123623002537" target="_blank">Journal of Functional Analysis</a>
					<a href="https://arxiv.org/abs/2210.08314" target="_blank">ArXiV</a>
				</div>

			
			</div>
			<div class="column">
				
				<div class="paper" id="blurop">
					<div class="formula">\(\boldsymbol{B_\mu^\varphi}\)</div>
					<div class="title">On a time-frequency blurring operator with applications in data augmentation</div>
					<div class="subtitle">May 2024</div>
					<a href="https://arxiv.org/abs/2405.12899" target="_blank">ArXiV</a>
					<a href="https://github.com/SimonHalvdansson/Time-Frequency-Blurring-Operator" target="_blank">GitHub</a>
				</div>
				
				<div class="paper" id="symbolrecovery">
					<div class="formula">\(\boldsymbol{A_f^g \mapsto f}\)</div>
					<div class="title">Five ways to recover the symbol of a non-binary localization operator</div>
					<div class="subtitle">January 2023</div>
					<a href="https://arxiv.org/abs/2301.11618" target="_blank">ArXiV</a>
					<a href="https://github.com/SimonHalvdansson/Localization-Operator-Symbol-Recovery" target="_blank">GitHub</a>
				</div>
				
				<div class="paper" id="minimizers">
					<div class="formula">\(\boldsymbol{\operatorname{argmin}_f \mathcal{L}_{\mathrm{P}}(f)}\)</div>
					<div class="title">Existence of Uncertainty Minimizers for the Continuous Wavelet Transform</div>
					<div class="subtitle">August 2021<br>with Jan-Fredrik Olsen, Ron Levie and Nir Sochen</div>
					<a href="https://onlinelibrary.wiley.com/doi/full/10.1002/mana.202100466">Mathematische Nachrichten</a>
					<a href="https://arxiv.org/abs/2108.12018" target="_blank">ArXiV</a>
				</div>

			</div>
		</div>
	</section>
	
	<div class="centering">
		<hr class="squiggly-line">
	</div>

	<section id="talks" class="centering">
		<h1>Talks/posters</h1>

		<div class="timeline">
			
			<div class="talk">
				<div class="name">Workshop on Quantum Harmonic Analysis (<a href="https://www.analysis.uni-hannover.de/de/institut/personenverzeichnis/robert-fulsche/qha" target="_blank">QHA2024</a>)</div>
				<div class="date_loc">August 5-9, 2024 · Hannover, Germany</div>
				<div class="link"><a href="slides/QHA24_Presentation.pdf" target="_blank">Slides</a></div>
			</div>

			<div class="talk">
				<div class="name">More on Harmonic Analysis (<a href="https://ps-mathematik.univie.ac.at/e/?event=strobl24" target="_blank">Strobl24</a>) </div>
				<div class="date_loc">June 9-15, 2024 · Strobl, Austria</div>
				<div class="link"><a href="slides/Strobl_poster_2024.pdf" target="_blank">Poster</a></div>
			</div>

			<div class="talk">
				<div class="name">International Workshop on Operator Theory and its Applications (<a href="https://www.helsinki.fi/en/conferences/iwota2023" target="_blank">IWOTA 2023</a>)</div>
				<div class="date_loc">July 31 - August 4, 2023 · Helsinki, Finland</div>
				<div class="link"><a href="slides/IWOTA_2023_Talk.pdf" target="_blank">Slides</a></div>
			</div>
	
			<div class="talk">
				<div class="name">Workshop on Quantum Harmonic Analysis (<a href="https://web.archive.org/web/20230922014919/https://www.ntnu.edu/imf/qha2023" target="_blank">QHA2023</a>)</div>
				<div class="date_loc">June 5-9, 2023 · NTNU Trondheim, Norway</div>
				<div class="link"><a href="slides/QHA23_Presentation.pdf" target="_blank">Slides</a></div>
			</div>
			
			<div class="talk">
				<div class="name">Dynamics in the noncommutative world (<a href="https://sites.google.com/view/dncw2023" target="_blank">DNCW 2023</a>)</div>
				<div class="date_loc">January 26, 2023 · NTNU Trondheim, Norway</div>
				<div class="link"><a href="slides/DNCW_Presentation_2023.pdf" target="_blank">Slides</a></div>
			</div>
	
			<div class="talk">
				<div class="name">Oberseminar Analysis</div>
				<div class="date_loc">October 4, 2022 · Leibniz Universität Hannover, Germany</div>
				<div class="link"><a href="slides/Hannover_Presentation_2022.pdf" target="_blank">Slides</a></div>
			</div>
	
			<div class="talk">
				<div class="name">International Conference on Computational Harmonic Analysis (<a href="https://ps-mathematik.univie.ac.at/e/index.php?event=ICCHA2022" target="_blank">ICCHA 2022</a>)</div>
				<div class="date_loc">September 12-16, 2022 · Ingolstadt, Germany</div>
				<div class="link"><a href="slides/ICCHA_Presentation_2022.pdf" target="_blank">Slides</a></div>
			</div>
	
			<div class="talk">
				<div class="name">International Workshop on Operator Theory and its Applications (<a href="https://iwota2022.urk.edu.pl/" target="_blank">IWOTA 2022</a>)</div>
				<div class="date_loc">September 6-10, 2022 · Krakow, Poland</div>
				<div class="link"><a href="slides/IWOTA_Presentation_2022.pdf" target="_blank">Slides</a></div>
			</div>
	
			<div class="talk">
				<div class="name">Applied Harmonic Analysis and Friends (<a href="https://ps-mathematik.univie.ac.at/e/?event=strobl22" target="_blank">Strobl22</a>)</div>
				<div class="date_loc">June 19-25, 2022 · Strobl, Austria</div>
				<div class="link"><a href="slides/Strobl22_poster.pdf" target="_blank">Poster</a></div>
			</div>
	
			<div class="talk">
				<div class="name">Online International Conference on Computational Harmonic Analysis (<a href="https://ps-mathematik.univie.ac.at/e/index.php?event=ICCHA2021online" target="_blank">ICCHA 2021</a>)</div>
				<div class="date_loc">September 13-17, 2021 · Online</div>
				<div class="link"><a href="slides/ICCHA2021_slides.pdf" target="_blank">Slides</a></div>
			</div>
		</div>

	</section>

</body>
</html>